dataset:
  root: /raid/xql/DIV2K/

  train:  # use LMDB
    type: DIV2KTrainingSet
    
    # for preparing lmdb
    gt_folder: DIV2K_train_HR_crop/
    lq_folder: DIV2K_train_HR_intra/qp37/
    
    # for defining dataset
    gt_path: div2k_train_gt.lmdb
    lq_path: div2k_train_lq_intra_qp37.lmdb
    gt_size: 128  # ground truth patch size: gt_size * gt_size
    use_flip: True
    use_rot: True  # rotation per 90 degrees

    # for defining datasampler
    enlarge_ratio: 100  # enlarge dataset by randomly cropping.
    
    # for defining dataloader
    num_worker_per_gpu: 32  # 32 in total. mainly affect IO speed
    batch_size_per_gpu: 32  # 32 in total
  
  val:  # use disk IO
    type: DIV2KTestSet
    gt_path: DIV2K_valid_HR_crop/
    lq_path: DIV2K_valid_HR_intra/qp37/

network:
  num_in_ch: 3
  num_feat: 32
  num_block: 8  # 23 originally. 30GB memory for bs=8 per gpu...
  num_out_ch: 3
  num_grow_ch: 16

train:
  exp_name: TrainRRDBNetDIV2KIntraQP37  # default: timestr. None: ~
  random_seed: 7
  num_iter: !!float 4e+5
  interval_print: !!float 100
  interval_val: !!float 1e+4  # also save model
  pbar_len: 100

  optim:
    type: Adam
    lr: !!float 2e-4  # init lr of scheduler
    betas: [0.9, 0.999]
    eps: !!float 1e-08

  scheduler:
    is_on: False
    type: CosineAnnealingRestartLR
    periods: [100000, 100000, 100000, 100000]  # epoch interval
    restart_weights: [1, 1, 1, 1]
    eta_min: !!float 1e-7

  loss:
    type: CharbonnierLoss
    eps: !!float 1e-6

  criterion:
    type: PSNR
    unit: dB
